group SizzleJavaHadoop : SizzleJava;

Program(name, inputFormatClass, keyClass, valueClass, staticDeclarations, staticStatements, statements, tables) ::= <<
package sizzle;

public class <name> extends sizzle.runtime.SizzleRunner {
	/** {@inheritDoc} */
	@Override
	public org.apache.hadoop.mapreduce.Job job(org.apache.hadoop.conf.Configuration configuration, org.apache.hadoop.fs.Path out, boolean robust) throws java.io.IOException {
		org.apache.hadoop.mapreduce.Job job = super.job(configuration, out, robust);

		job.getConfiguration().setBoolean("mapred.map.tasks.speculative.execution", false);
		job.getConfiguration().setBoolean("mapred.reduce.tasks.speculative.execution", false);
		job.getConfiguration().setLong("mapred.job.reuse.jvm.num.tasks", -1);

		job.setInputFormatClass(<inputFormatClass>.class);
		job.setJobName("<name>: " + out);
		job.setJarByClass(<name>SizzleMapper.class);
		
		job.setMapperClass(<name>SizzleMapper.class);
		job.setCombinerClass(<name>SizzleCombiner.class);
		job.setReducerClass(<name>SizzleReducer.class);
		
		return job;
	}
	
	public static void main(String[] args) throws java.io.IOException, InterruptedException, ClassNotFoundException {
		<name> runner = new <name>();
		
		org.apache.commons.cli.CommandLine line = parseArgs(args, runner.getUsage());
		args = line.getArgs();
		if (args.length != 2) {
			System.err.println("Wrong number of arguments.");
			printHelp(runner.getUsage());
		}

		final boolean robust = line.hasOption("robust");
		int id = 0;
		if (line.hasOption("job"))
			id = Integer.parseInt(line.getOptionValue("job"));
		
		org.apache.hadoop.mapreduce.Job jb = runner.job(org.apache.hadoop.hbase.HBaseConfiguration.create(), new org.apache.hadoop.fs.Path(args[1]), robust);
		jb.getConfiguration().set("boa.hbase.table", args[0]);

		org.apache.hadoop.hbase.client.Scan scan = new org.apache.hadoop.hbase.client.Scan();
		scan.setCaching(500);
		scan.setCacheBlocks(false);
		scan.setFilter(new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.NOT_EQUAL, new org.apache.hadoop.hbase.filter.SubstringComparator("!!")));
		scan.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes("metadata"), org.apache.hadoop.hbase.util.Bytes.toBytes("proto"));
		org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob(org.apache.hadoop.hbase.util.Bytes.toBytes(args[0]), scan, <name>SizzleMapper.class, sizzle.io.EmitKey.class, sizzle.io.EmitValue.class, jb);

		if (id > 0) {
			jb.getConfiguration().setInt("boa.hadoop.jobid", id);
			jb.getConfiguration().set("boa.hadoop.output", args[1]);
		}
		jb.submit();
		if (id > 0)
			sizzle.io.SizzleOutputCommitter.setJobID(jb.getJobID().toString(), id);
		System.err.println("Job ID: " + jb.getJobID().toString());

		if (line.hasOption("block")) {
			while (!jb.isComplete()) {
				if (jb.setupProgress() \< 1)
					System.err.println("SETUP: " + (jb.setupProgress() * 100) + "%");
				else if (jb.mapProgress() \< 1)
					System.err.println("MAP: " + (jb.mapProgress() * 100) + "%");
				else
					System.err.println("REDUCE: " + (jb.reduceProgress() * 100) + "%");
				try {
					Thread.sleep (1000);
				} catch (Exception e) {}
			}
			System.err.println("JOB FINISHED: " + (jb.isSuccessful() ? "Success" : "Failed"));
		}
	}

	public String getUsage() {
		return "\<HBaseInputTableName\> \<outputDir\>";
	}
	
	static class <name>SizzleMapper extends sizzle.runtime.SizzleTableMapper {
		<staticDeclarations>
		
		<if(staticStatements)>
		{
			<staticStatements>
		}
		
		<endif>
		/** {@inheritDoc} */
		@Override
		protected void map(org.apache.hadoop.hbase.io.ImmutableBytesWritable row, org.apache.hadoop.hbase.client.Result columns, org.apache.hadoop.mapreduce.Mapper.Context context) throws java.io.IOException {
			try {
				byte[] value = null;
				for (org.apache.hadoop.hbase.KeyValue kv : columns.list())
					value = kv.getValue();

				com.google.protobuf.CodedInputStream _inputStream = com.google.protobuf.CodedInputStream.newInstance(value, 0, value.length);
				<statements:{<it><\n>}>
			} catch (java.io.IOException e) {
				throw e;
			} catch (RuntimeException e) {
				if (this.robust)
					LOG.error(e.getClass().getName() + " caught", e);
				else
					throw e;
			} catch (Exception e) {
				if (this.robust)
					LOG.error(e.getClass().getName() + " caught", e);
				else
					throw new RuntimeException(e.getClass().getName() + " caught", e);
			}
		}

		/** {@inheritDoc} */
		@Override
		protected void setup(org.apache.hadoop.mapreduce.Mapper.Context context) throws java.io.IOException, java.lang.InterruptedException {
			sizzle.functions.BoaAstIntrinsics.initialize(context);
			super.setup(context);
		}

		/** {@inheritDoc} */
		@Override
		protected void cleanup(org.apache.hadoop.mapreduce.Mapper.Context context) throws java.io.IOException, java.lang.InterruptedException {
			sizzle.functions.BoaAstIntrinsics.close();
			super.cleanup(context);
		}
	}
	
	static class <name>SizzleCombiner extends sizzle.runtime.SizzleCombiner {
		public <name>SizzleCombiner() {
			super();
			
			<tables:{<it><\n>}>
		}
	}
	
	static class <name>SizzleReducer extends sizzle.runtime.SizzleReducer {
		public <name>SizzleReducer() {
			super();


			<tables:{<it><\n>}>
		}
	}
	
	@Override
	public org.apache.hadoop.mapreduce.Mapper getMapper() {
		return new <name>SizzleMapper();
	}
	
	@Override
	public sizzle.runtime.SizzleCombiner getCombiner() {
		return new <name>SizzleCombiner();
	}
	
	@Override
	public sizzle.runtime.SizzleReducer getReducer() {
		return new <name>SizzleReducer();
	}
}
>>

EmitStatement(indices, id, expression, weight) ::= "context.write(new sizzle.io.EmitKey(<if(indices)><indices:{\"[\" + <it> + \"]\"}; separator=\" + \">, <endif><id>), new sizzle.io.EmitValue(<expression><if(weight)>, <weight><endif>));<\n>"
